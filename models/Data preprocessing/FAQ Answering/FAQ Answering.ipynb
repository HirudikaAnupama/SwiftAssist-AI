{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T00:37:24.356116Z",
     "start_time": "2025-03-21T20:38:15.204721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast  # For mixed precision\n",
    "from torch.utils.data import DataLoader, Dataset  # For parallel data loading\n",
    "\n",
    "# Start total timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to('cuda')\n",
    "\n",
    "# Clean question function\n",
    "def clean_question(text):\n",
    "    # Lowercase and remove punctuation\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "\n",
    "    # Correct typos (optional)\n",
    "    words = text.split()\n",
    "    corrected_words = [spell.correction(word) if spell.correction(word) is not None else word for word in words]\n",
    "    text = ' '.join(corrected_words)\n",
    "\n",
    "    # Lemmatize and remove stopwords\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Clean answer function\n",
    "def clean_answer(text):\n",
    "    # Trim whitespace\n",
    "    text = text.strip()\n",
    "    # Remove markdown/HTML tags (example)\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'\\*\\*', '', text)  # Remove bold markers\n",
    "    return text\n",
    "\n",
    "# Dataset class for parallel data loading\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "# Generate embeddings in batches\n",
    "def generate_embeddings_batch(texts, batch_size=128):\n",
    "    embeddings = []\n",
    "    dataset = TextDataset(texts)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=0, shuffle=False)  # Set num_workers=0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "        with torch.no_grad(), autocast():  # Mixed precision\n",
    "            outputs = model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "D2 = pd.read_csv('data/FAQ Answering/Preprocessed data/D2.csv')\n",
    "\n",
    "# Rename columns\n",
    "D2 = D2.rename(columns={'query': 'question', 'finalpassage': 'answer'})\n",
    "\n",
    "# Apply cleaning functions\n",
    "print(\"Cleaning data...\")\n",
    "D2[\"cleaned_query\"] = [clean_question(q) for q in tqdm(D2[\"question\"], desc=\"Cleaning questions\")]\n",
    "D2[\"cleaned_answer\"] = [clean_answer(a) for a in tqdm(D2[\"answer\"], desc=\"Cleaning answers\")]\n",
    "\n",
    "# Generate embeddings for cleaned queries in batches\n",
    "print(\"Generating embeddings...\")\n",
    "D2[\"embedding\"] = generate_embeddings_batch(D2[\"cleaned_query\"].tolist(), batch_size=128)\n",
    "\n",
    "# Save embeddings to disk\n",
    "print(\"Saving data...\")\n",
    "D2.to_csv(\"data/FAQ Answering/Preprocessed embedding/D2_emb.csv\", index=False)\n",
    "\n",
    "# End total timer\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total execution time: {total_time:.2f} seconds\")"
   ],
   "id": "429d88efa03a9392",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Cleaning data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning questions: 100%|██████████| 130560/130560 [3:54:12<00:00,  9.29it/s]  \n",
      "Cleaning answers: 100%|██████████| 130560/130560 [00:00<00:00, 254262.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|          | 0/1020 [00:00<?, ?it/s]C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_8920\\1373753907.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():  # Mixed precision\n",
      "Generating embeddings: 100%|██████████| 1020/1020 [00:17<00:00, 57.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "Total execution time: 14349.12 seconds\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:34:50.260023Z",
     "start_time": "2025-03-21T20:34:41.461939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast  # For mixed precision\n",
    "from torch.utils.data import DataLoader, Dataset  # For parallel data loading\n",
    "\n",
    "# Start total timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to('cuda')\n",
    "\n",
    "# Clean question function\n",
    "def clean_question(text):\n",
    "    # Lowercase and remove punctuation\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "\n",
    "    # Correct typos (optional)\n",
    "    words = text.split()\n",
    "    corrected_words = [spell.correction(word) if spell.correction(word) is not None else word for word in words]\n",
    "    text = ' '.join(corrected_words)\n",
    "\n",
    "    # Lemmatize and remove stopwords\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Clean answer function\n",
    "def clean_answer(text):\n",
    "    # Trim whitespace\n",
    "    text = text.strip()\n",
    "    # Remove markdown/HTML tags (example)\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'\\*\\*', '', text)  # Remove bold markers\n",
    "    return text\n",
    "\n",
    "# Dataset class for parallel data loading\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "# Generate embeddings in batches\n",
    "def generate_embeddings_batch(texts, batch_size=128):\n",
    "    embeddings = []\n",
    "    dataset = TextDataset(texts)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=0, shuffle=False)  # Set num_workers=0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "        with torch.no_grad(), autocast():  # Mixed precision\n",
    "            outputs = model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "D3 = pd.read_csv('data/FAQ Answering/Preprocessed data/D3.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Apply cleaning functions\n",
    "print(\"Cleaning data...\")\n",
    "D3[\"cleaned_query\"] = [clean_question(q) for q in tqdm(D3[\"question\"], desc=\"Cleaning questions\")]\n",
    "D3[\"cleaned_answer\"] = [clean_answer(a) for a in tqdm(D3[\"answer\"], desc=\"Cleaning answers\")]\n",
    "\n",
    "# Generate embeddings for cleaned queries in batches\n",
    "print(\"Generating embeddings...\")\n",
    "D3[\"embedding\"] = generate_embeddings_batch(D3[\"cleaned_query\"].tolist(), batch_size=128)\n",
    "\n",
    "# Save embeddings to disk\n",
    "print(\"Saving data...\")\n",
    "D3.to_csv(\"data/FAQ Answering/Preprocessed embedding/D3_emb.csv\", index=False)\n",
    "\n",
    "# End total timer\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total execution time: {total_time:.2f} seconds\")"
   ],
   "id": "391d989e291ca873",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Cleaning data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning questions: 100%|██████████| 414/414 [00:04<00:00, 98.82it/s] \n",
      "Cleaning answers: 100%|██████████| 414/414 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|          | 0/4 [00:00<?, ?it/s]C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_8920\\1463753038.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():  # Mixed precision\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "Total execution time: 8.77 seconds\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T19:53:29.504038Z",
     "start_time": "2025-03-21T19:53:29.491830Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "821eaebcf5ed8246",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T19:55:02.142357Z",
     "start_time": "2025-03-21T19:54:54.899066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast  # For mixed precision\n",
    "from torch.utils.data import DataLoader, Dataset  # For parallel data loading\n",
    "\n",
    "# Start total timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to('cuda')\n",
    "\n",
    "# Clean question function\n",
    "def clean_question(text):\n",
    "    # Lowercase and remove punctuation\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "\n",
    "    # Correct typos (optional)\n",
    "    words = text.split()\n",
    "    corrected_words = [spell.correction(word) if spell.correction(word) is not None else word for word in words]\n",
    "    text = ' '.join(corrected_words)\n",
    "\n",
    "    # Lemmatize and remove stopwords\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Clean answer function\n",
    "def clean_answer(text):\n",
    "    # Trim whitespace\n",
    "    text = text.strip()\n",
    "    # Remove markdown/HTML tags (example)\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'\\*\\*', '', text)  # Remove bold markers\n",
    "    return text\n",
    "\n",
    "# Dataset class for parallel data loading\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "# Generate embeddings in batches\n",
    "def generate_embeddings_batch(texts, batch_size=128):\n",
    "    embeddings = []\n",
    "    dataset = TextDataset(texts)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=0, shuffle=False)  # Set num_workers=0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "        with torch.no_grad(), autocast():  # Mixed precision\n",
    "            outputs = model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "D5 = pd.read_csv('data/FAQ Answering/Preprocessed data/D5.csv')\n",
    "\n",
    "# Rename columns\n",
    "D5 = D5.rename(columns={'query': 'question', 'finalpassage': 'answer'})\n",
    "\n",
    "# Apply cleaning functions\n",
    "print(\"Cleaning data...\")\n",
    "D5[\"cleaned_query\"] = [clean_question(q) for q in tqdm(D5[\"question\"], desc=\"Cleaning questions\")]\n",
    "D5[\"cleaned_answer\"] = [clean_answer(a) for a in tqdm(D5[\"answer\"], desc=\"Cleaning answers\")]\n",
    "\n",
    "# Generate embeddings for cleaned queries in batches\n",
    "print(\"Generating embeddings...\")\n",
    "D5[\"embedding\"] = generate_embeddings_batch(D5[\"cleaned_query\"].tolist(), batch_size=128)\n",
    "\n",
    "# Save embeddings to disk\n",
    "print(\"Saving data...\")\n",
    "D5.to_csv(\"data/FAQ Answering/Preprocessed embedding/D5_emb.csv\", index=False)\n",
    "\n",
    "# End total timer\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total execution time: {total_time:.2f} seconds\")"
   ],
   "id": "e446c5b43e586a7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Cleaning data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning questions: 100%|██████████| 200/200 [00:03<00:00, 50.20it/s] \n",
      "Cleaning answers: 100%|██████████| 200/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|          | 0/2 [00:00<?, ?it/s]C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_8920\\811628211.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():  # Mixed precision\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "Total execution time: 7.22 seconds\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a52eb6f68660a28a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
