{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-16T20:25:19.231479Z",
     "start_time": "2025-03-16T20:25:19.176670Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"data/Intent recognition/Preprocessed data/Data.csv\")  # Replace with your dataset file\n",
    "print(data.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text         intent\n",
      "0   listen to westbam alumb allergic on google music      playmusic\n",
      "1         add step to me to the 50 clÃ¡sicos playlist  addtoplaylist\n",
      "2  i give this current textbook a rating value of...       ratebook\n",
      "3               play the song little robin redbreast      playmusic\n",
      "4  please add iris dement to my playlist this is ...  addtoplaylist\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T20:25:26.233684Z",
     "start_time": "2025-03-16T20:25:25.739087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text  # Handle non-string values\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to text and intent columns\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "data['intent'] = data['intent'].apply(clean_text)\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())"
   ],
   "id": "a762789d13877e38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text      0\n",
      "intent    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T21:14:17.446670Z",
     "start_time": "2025-03-16T21:14:17.422841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Encode intent labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['intent_encoded'] = label_encoder.fit_transform(data['intent'])\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "# Save the LabelEncoder to a file\n",
    "joblib.dump(label_encoder, 'models/Model training/Intent recognition/label_encoder.pkl')  # Save it in the current directory"
   ],
   "id": "5b22e896ea8b5816",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text         intent  \\\n",
      "0   listen to westbam alumb allergic on google music      playmusic   \n",
      "1         add step to me to the 50 clÃ¡sicos playlist  addtoplaylist   \n",
      "2  i give this current textbook a rating value of...       ratebook   \n",
      "3               play the song little robin redbreast      playmusic   \n",
      "4  please add iris dement to my playlist this is ...  addtoplaylist   \n",
      "\n",
      "   intent_encoded  \n",
      "0              35  \n",
      "1               0  \n",
      "2              38  \n",
      "3              35  \n",
      "4               0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/Model training/Intent recognition/label_encoder.pkl']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T20:26:02.584528Z",
     "start_time": "2025-03-16T20:26:02.445598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(train_df)} examples\")\n",
    "print(f\"Validation set: {len(val_df)} examples\")\n",
    "print(f\"Test set: {len(test_df)} examples\")"
   ],
   "id": "a06024fd3f4c9ed4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 33657 examples\n",
      "Validation set: 3740 examples\n",
      "Test set: 9350 examples\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T20:26:33.213733Z",
     "start_time": "2025-03-16T20:26:10.470052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text data\n",
    "def tokenize_data(df, tokenizer, max_length=64):\n",
    "    return tokenizer(\n",
    "        df['text'].tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize training, validation, and test data\n",
    "train_encodings = tokenize_data(train_df, tokenizer)\n",
    "val_encodings = tokenize_data(val_df, tokenizer)\n",
    "test_encodings = tokenize_data(test_df, tokenizer)"
   ],
   "id": "6e5810f985ef30c8",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T20:26:39.251573Z",
     "start_time": "2025-03-16T20:26:39.234296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "class IntentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = IntentDataset(train_encodings, train_df['intent_encoded'].tolist())\n",
    "val_dataset = IntentDataset(val_encodings, val_df['intent_encoded'].tolist())\n",
    "test_dataset = IntentDataset(test_encodings, test_df['intent_encoded'].tolist())"
   ],
   "id": "7fbd76a2a47e2040",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T20:26:53.010084Z",
     "start_time": "2025-03-16T20:26:44.870814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load pre-trained BERT for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(label_encoder.classes_)  # Number of unique intents\n",
    ")"
   ],
   "id": "b5eb59ad3b686235",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T20:26:57.263020Z",
     "start_time": "2025-03-16T20:26:53.010084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # Output directory\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation\n",
    "    warmup_steps=500,  # Number of warmup steps\n",
    "    weight_decay=0.01,  # Strength of weight decay\n",
    "    logging_dir='./logs',  # Directory for storing logs\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    evaluation_strategy=\"epoch\"  # Evaluate after each epoch\n",
    ")"
   ],
   "id": "8ae3f9bde77febc0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\miniconda3\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T20:27:10.653816Z",
     "start_time": "2025-03-16T20:27:09.513450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")"
   ],
   "id": "499bc516be20c29",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\nexpected string or bytes-like object, got 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1817\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1816\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1817\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1818\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\importlib\\__init__.py:90\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m     89\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1387\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1360\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1331\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:935\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:995\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:488\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\trainer.py:195\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_datasets_available():\n\u001B[1;32m--> 195\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mdatasets\u001B[39;00m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torch_xla_available():\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\datasets\\__init__.py:17\u001B[0m\n\u001B[0;32m     15\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m3.2.0\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_dataset\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_reader\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ReadInstruction\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:75\u001B[0m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontrib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconcurrent\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m thread_map\n\u001B[1;32m---> 75\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_reader\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ArrowReader\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\datasets\\config.py:37\u001B[0m\n\u001B[0;32m     36\u001B[0m DILL_VERSION \u001B[38;5;241m=\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(importlib\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mversion(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdill\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m---> 37\u001B[0m FSSPEC_VERSION \u001B[38;5;241m=\u001B[39m \u001B[43mversion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mversion\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfsspec\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m PANDAS_VERSION \u001B[38;5;241m=\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(importlib\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mversion(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpandas\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\packaging\\version.py:56\u001B[0m, in \u001B[0;36mparse\u001B[1;34m(version)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Parse the given version string.\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \n\u001B[0;32m     50\u001B[0m \u001B[38;5;124;03m>>> parse('1.0.dev1')\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;124;03m:raises InvalidVersion: When the version string is not a valid version.\u001B[39;00m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m---> 56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVersion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mversion\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\packaging\\version.py:200\u001B[0m, in \u001B[0;36mVersion.__init__\u001B[1;34m(self, version)\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# Validate the version and parse it into pieces\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m match \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_regex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mversion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m match:\n",
      "\u001B[1;31mTypeError\u001B[0m: expected string or bytes-like object, got 'NoneType'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[70], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Trainer\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Define Trainer\u001B[39;00m\n\u001B[0;32m      4\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m      5\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m      6\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m      7\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mtrain_dataset,\n\u001B[0;32m      8\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39mval_dataset\n\u001B[0;32m      9\u001B[0m )\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1412\u001B[0m, in \u001B[0;36m_handle_fromlist\u001B[1;34m(module, fromlist, import_, recursive)\u001B[0m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1805\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1803\u001B[0m     value \u001B[38;5;241m=\u001B[39m Placeholder\n\u001B[0;32m   1804\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m-> 1805\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1806\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[0;32m   1807\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1819\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1817\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m   1818\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m-> 1819\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1820\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1821\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1822\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nexpected string or bytes-like object, got 'NoneType'"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T20:27:16.380402Z",
     "start_time": "2025-03-16T20:27:16.349716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ],
   "id": "c65f6d5beed4e522",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[71], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241m.\u001B[39mtrain()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T20:27:20.159580Z",
     "start_time": "2025-03-16T20:27:20.123058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate on the test set\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(results)"
   ],
   "id": "ccccb7fc304786e4",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[72], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Evaluate on the test set\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241m.\u001B[39mevaluate(test_dataset)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(results)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T20:27:27.138390Z",
     "start_time": "2025-03-16T20:27:24.790232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./intent_recognition_bert')\n",
    "tokenizer.save_pretrained('./intent_recognition_bert')"
   ],
   "id": "8dfbbd9ed60247f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./intent_recognition_bert\\\\tokenizer_config.json',\n",
       " './intent_recognition_bert\\\\special_tokens_map.json',\n",
       " './intent_recognition_bert\\\\vocab.txt',\n",
       " './intent_recognition_bert\\\\added_tokens.json')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('./intent_recognition_bert')\n",
    "tokenizer = BertTokenizer.from_pretrained('./intent_recognition_bert')\n",
    "\n",
    "# Predict intent for a real-time query\n",
    "def predict_intent(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
    "    outputs = model(**inputs)\n",
    "    predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return label_encoder.inverse_transform([predicted_label])[0]\n",
    "\n",
    "# Test with a real-time query\n",
    "user_query = \"What's the weather like today?\"\n",
    "predicted_intent = predict_intent(user_query)\n",
    "print(f\"Predicted Intent: {predicted_intent}\")"
   ],
   "id": "443b5356861afe8f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
