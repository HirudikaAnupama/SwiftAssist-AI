{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"/content/Data_1.csv\")\n",
        "\n",
        "# Remove neutral samples (sentiment=1)\n",
        "df = df[df[\"sentiment\"] != 1]\n",
        "\n",
        "# Remap labels: 0 = negative, 2 = positive ‚Üí 0 = negative, 1 = positive\n",
        "df[\"sentiment\"] = df[\"sentiment\"].replace(2, 1)\n",
        "\n",
        "# Check distribution\n",
        "print(df[\"sentiment\"].value_counts())\n",
        "# Expected output:\n",
        "# 0    527,381 (negative)\n",
        "# 1    584,436 (positive)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqg45u6IAwxz",
        "outputId": "1cd5d505-f60c-46e7-c1d3-2258f4768986"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment\n",
            "1    584436\n",
            "0    527381\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "texts = df[\"text\"].tolist()\n",
        "labels = df[\"sentiment\"].tolist()\n",
        "\n",
        "# Stratified split (80% train, 10% val, 10% test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "sNqHX9HVB_1E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Compute class weights correctly\n",
        "classes = np.unique(y_train)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=classes,\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Initialize model with proper config\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    id2label={0: \"negative\", 1: \"positive\"},\n",
        "    label2id={\"negative\": 0, \"positive\": 1}\n",
        ").to(device)\n",
        "\n",
        "# Optimized training setup\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Enhanced Dataset class with caching\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=96):  # Reduced max_len\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.encodings = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.encodings is None:\n",
        "            text = str(self.texts[idx])\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                max_length=self.max_len,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            return {\n",
        "                \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "                \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "                \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "            }\n",
        "        else:\n",
        "            return self.encodings[idx]\n",
        "\n",
        "# Create datasets with optimized tokenization\n",
        "train_dataset = SentimentDataset(X_train, y_train, tokenizer)\n",
        "val_dataset = SentimentDataset(X_val, y_val, tokenizer)\n",
        "\n",
        "# Use larger batch sizes with gradient accumulation\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128)\n",
        "\n",
        "# Training loop with mixed precision\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "grad_accum_steps = 2\n",
        "\n",
        "for epoch in range(3):\n",
        "    # Training\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")\n",
        "\n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "            loss = loss / grad_accum_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % grad_accum_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * grad_accum_steps\n",
        "        avg_loss = total_loss / (step + 1)\n",
        "        progress_bar.set_postfix({\"loss\": f\"{avg_loss:.4f}\"})\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    val_accuracy = (np.array(val_preds) == np.array(val_labels)).mean()\n",
        "    val_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Results:\")\n",
        "    print(f\"Train Loss: {avg_loss:.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "    print(f\"Val Accuracy: {val_accuracy:.4f} | Val F1: {val_f1:.4f}\")\n",
        "    print(classification_report(val_labels, val_preds, target_names=[\"negative\", \"positive\"]))\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7TRtYnaCDCw",
        "outputId": "5c397204-e871-4158-8429-4e079746f49e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "<ipython-input-7-b1cb8a43d252>:70: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "Epoch 1 [Train]:   0%|          | 0/13898 [00:00<?, ?it/s]<ipython-input-7-b1cb8a43d252>:83: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch 1 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13898/13898 [32:21<00:00,  7.16it/s, loss=0.4331]\n",
            "Epoch 1 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 869/869 [05:22<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Results:\n",
            "Train Loss: 0.4331 | Val Loss: 0.4189\n",
            "Val Accuracy: 0.8031 | Val F1: 0.8031\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.76      0.86      0.80     52738\n",
            "    positive       0.85      0.76      0.80     58444\n",
            "\n",
            "    accuracy                           0.80    111182\n",
            "   macro avg       0.81      0.81      0.80    111182\n",
            "weighted avg       0.81      0.80      0.80    111182\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Train]:   0%|          | 0/13898 [00:00<?, ?it/s]<ipython-input-7-b1cb8a43d252>:83: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch 2 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13898/13898 [32:12<00:00,  7.19it/s, loss=0.3831]\n",
            "Epoch 2 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 869/869 [05:23<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Results:\n",
            "Train Loss: 0.3831 | Val Loss: 0.4084\n",
            "Val Accuracy: 0.8140 | Val F1: 0.8134\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.81      0.80      0.80     52738\n",
            "    positive       0.82      0.83      0.82     58444\n",
            "\n",
            "    accuracy                           0.81    111182\n",
            "   macro avg       0.81      0.81      0.81    111182\n",
            "weighted avg       0.81      0.81      0.81    111182\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [Train]:   0%|          | 0/13898 [00:00<?, ?it/s]<ipython-input-7-b1cb8a43d252>:83: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch 3 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13898/13898 [32:12<00:00,  7.19it/s, loss=0.3413]\n",
            "Epoch 3 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 869/869 [05:23<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 Results:\n",
            "Train Loss: 0.3413 | Val Loss: 0.4229\n",
            "Val Accuracy: 0.8130 | Val F1: 0.8124\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.81      0.80      0.80     52738\n",
            "    positive       0.82      0.83      0.82     58444\n",
            "\n",
            "    accuracy                           0.81    111182\n",
            "   macro avg       0.81      0.81      0.81    111182\n",
            "weighted avg       0.81      0.81      0.81    111182\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model and tokenizer\n",
        "model.save_pretrained(\"./sentiment_model\")\n",
        "tokenizer.save_pretrained(\"./sentiment_model\")\n",
        "\n",
        "# Optional: Save PyTorch weights\n",
        "torch.save(model.state_dict(), \"./sentiment_model/pytorch_model.bin\")"
      ],
      "metadata": {
        "id": "n1AScJnzCHVL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load from saved directory\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./sentiment_model\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./sentiment_model\")"
      ],
      "metadata": {
        "id": "YkwDBO14EbUh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        max_length=96,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoding)\n",
        "        probs = torch.softmax(outputs.logits, dim=1)\n",
        "        pred = torch.argmax(probs).item()\n",
        "\n",
        "    return \"positive\" if pred == 1 else \"negative\", probs.cpu().numpy()[0]"
      ],
      "metadata": {
        "id": "8xSkIIhJEdYt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "    (\"This product is absolutely amazing! Worth every penny!\", \"positive\"),\n",
        "    (\"Worst customer service I've ever experienced.\", \"negative\"),\n",
        "    (\"The item arrived damaged but the replacement process was smooth.\", \"negative\"),  # Mixed sentiment\n",
        "    (\"Meh, it's okay I guess.\", \"negative\"),\n",
        "    (\"\", \"neutral\"),  # Edge case: empty input\n",
        "    (\"The\", \"neutral\"),  # Edge case: very short input\n",
        "]\n",
        "\n",
        "for text, expected in test_cases:\n",
        "    if not text.strip():  # Handle empty input\n",
        "        print(f\"Input: '{text}' => Error: Empty input\")\n",
        "        continue\n",
        "\n",
        "    pred, probs = predict_sentiment(text, model, tokenizer, device)\n",
        "    confidence = probs[1] if pred == \"positive\" else probs[0]\n",
        "    print(f\"Input: {text}\")\n",
        "    print(f\"Predicted: {pred} ({confidence:.2f}) | Expected: {expected}\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFktn4R6EgaR",
        "outputId": "97132d66-8ce1-4890-e5c0-55d6f50e4223"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: This product is absolutely amazing! Worth every penny!\n",
            "Predicted: positive (0.99) | Expected: positive\n",
            "--------------------------------------------------------------------------------\n",
            "Input: Worst customer service I've ever experienced.\n",
            "Predicted: negative (1.00) | Expected: negative\n",
            "--------------------------------------------------------------------------------\n",
            "Input: The item arrived damaged but the replacement process was smooth.\n",
            "Predicted: positive (0.87) | Expected: negative\n",
            "--------------------------------------------------------------------------------\n",
            "Input: Meh, it's okay I guess.\n",
            "Predicted: negative (0.69) | Expected: negative\n",
            "--------------------------------------------------------------------------------\n",
            "Input: '' => Error: Empty input\n",
            "Input: The\n",
            "Predicted: negative (0.61) | Expected: neutral\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_real_world(model, tokenizer, device):\n",
        "    # Test with different categories\n",
        "    categories = {\n",
        "        \"Sarcasm\": [\n",
        "            (\"Oh great, another broken feature!\", \"negative\"),\n",
        "            (\"Because I love waiting on hold for hours!\", \"negative\")\n",
        "        ],\n",
        "        \"Mixed Sentiments\": [\n",
        "            (\"The food was excellent but the service ruined everything.\", \"negative\"),\n",
        "            (\"Expensive but worth it for the quality.\", \"positive\")\n",
        "        ],\n",
        "        \"Emojis\": [\n",
        "            (\"üî•üî• Best purchase ever! üòç\", \"positive\"),\n",
        "            (\"Never again üò§üëé\", \"negative\")\n",
        "        ],\n",
        "        \"Typos\": [\n",
        "            (\"This produc is amezing!\", \"positive\"),\n",
        "            (\"Terible exprience!!!\", \"negative\")\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    for category, examples in categories.items():\n",
        "        print(f\"\\n{category} Testing:\")\n",
        "        for text, expected in examples:\n",
        "            pred, _ = predict_sentiment(text, model, tokenizer, device)\n",
        "            result = \"‚úì\" if pred == expected else \"‚úó\"\n",
        "            print(f\"{result} Text: {text}\")\n",
        "            print(f\"Predicted: {pred} | Expected: {expected}\")\n",
        "            print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "goG6vxvlElHG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find problematic predictions\n",
        "error_cases = []\n",
        "# Assuming you have labeled test data, create test_dataset here\n",
        "# Example:\n",
        "# test_dataset = SentimentDataset(X_test, y_test, tokenizer) # Replace X_test, y_test if necessary\n",
        "\n",
        "for text, true_label in test_dataset:\n",
        "    pred, _ = predict_sentiment(text, model, tokenizer, device)\n",
        "    if pred != true_label:\n",
        "        error_cases.append({\n",
        "            \"text\": text,\n",
        "            \"true\": true_label,\n",
        "            \"predicted\": pred,\n",
        "            \"length\": len(text.split()),\n",
        "            \"caps_ratio\": sum(1 for c in text if c.isupper())/len(text) if text else 0\n",
        "        })\n",
        "\n",
        "# Analyze error patterns\n",
        "import pandas as pd\n",
        "error_df = pd.DataFrame(error_cases)\n",
        "print(\"\\nError Analysis:\")\n",
        "print(error_df.describe())\n",
        "print(\"\\nCommon Characteristics:\")\n",
        "print(error_df.groupby([\"true\", \"predicted\"]).size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Y5CngZ_AEoJi",
        "outputId": "dbc02083-df93-4848-dffc-64b16d21852c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-857cc45b098b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# test_dataset = SentimentDataset(X_test, y_test, tokenizer) # Replace X_test, y_test if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtrue_label\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
          ]
        }
      ]
    }
  ]
}