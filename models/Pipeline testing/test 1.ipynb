{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Intent recognition",
   "id": "79689049035178c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T08:17:12.311702Z",
     "start_time": "2025-03-22T08:17:12.162608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Load the LabelEncoder\n",
    "label_encoder = joblib.load('models/Model training/Intent recognition/label_encoder.pkl')  # Replace with the path to your saved LabelEncoder\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('intent_recognition_bert')\n",
    "tokenizer = BertTokenizer.from_pretrained('intent_recognition_bert')"
   ],
   "id": "feafd49fdbd1ae4c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T08:17:13.078873Z",
     "start_time": "2025-03-22T08:17:13.072249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Mapping dictionary for intent labels\n",
    "intent_label_mapping = {\n",
    "    'playmusic': 'Play Music',\n",
    "    'addtoplaylist': 'Add to Playlist',\n",
    "    'ratebook': 'Rate Book',\n",
    "    'searchscreeningevent': 'Search Screening Event',\n",
    "    'bookrestaurant': 'Book Restaurant',\n",
    "    'getweather': 'Get Weather',\n",
    "    'searchcreativework': 'Search Creative Work',\n",
    "    'greeting': 'Greeting',\n",
    "    'greetingresponse': 'Greeting Response',\n",
    "    'courtesygreeting': 'Courtesy Greeting',\n",
    "    'courtesygreetingresponse': 'Courtesy Greeting Response',\n",
    "    'currenthumanquery': 'Current Human Query',\n",
    "    'namequery': 'Name Query',\n",
    "    'realnamequery': 'Real Name Query',\n",
    "    'timequery': 'Time Query',\n",
    "    'thanks': 'Thanks',\n",
    "    'nottalking2u': 'Not Talking to You',\n",
    "    'understandquery': 'Understand Query',\n",
    "    'shutup': 'Shut Up',\n",
    "    'swearing': 'Swearing',\n",
    "    'goodbye': 'Goodbye',\n",
    "    'courtesygoodbye': 'Courtesy Goodbye',\n",
    "    'whoami': 'Who Am I',\n",
    "    'clever': 'Clever',\n",
    "    'gossip': 'Gossip',\n",
    "    'jokes': 'Jokes',\n",
    "    'podbaydoor': 'Pod Bay Door',\n",
    "    'podbaydoorresponse': 'Pod Bay Door Response',\n",
    "    'selfaware': 'Self Aware',\n",
    "    'cancelorder': 'Cancel Order',\n",
    "    'changeorder': 'Change Order',\n",
    "    'changeshippingaddress': 'Change Shipping Address',\n",
    "    'checkcancellationfee': 'Check Cancellation Fee',\n",
    "    'checkinvoice': 'Check Invoice',\n",
    "    'checkpaymentmethods': 'Check Payment Methods',\n",
    "    'checkrefundpolicy': 'Check Refund Policy',\n",
    "    'complaint': 'Complaint',\n",
    "    'contactcustomerservice': 'Contact Customer Service',\n",
    "    'contacthumanagent': 'Contact Human Agent',\n",
    "    'createaccount': 'Create Account',\n",
    "    'deleteaccount': 'Delete Account',\n",
    "    'deliveryoptions': 'Delivery Options',\n",
    "    'deliveryperiod': 'Delivery Period',\n",
    "    'editaccount': 'Edit Account',\n",
    "    'getinvoice': 'Get Invoice',\n",
    "    'getrefund': 'Get Refund',\n",
    "    'newslettersubscription': 'Newsletter Subscription',\n",
    "    'paymentissue': 'Payment Issue',\n",
    "    'placeorder': 'Place Order',\n",
    "    'recoverpassword': 'Recover Password',\n",
    "    'registrationproblems': 'Registration Problems',\n",
    "    'review': 'Review',\n",
    "    'setupshippingaddress': 'Setup Shipping Address',\n",
    "    'switchaccount': 'Switch Account',\n",
    "    'trackorder': 'Track Order',\n",
    "    'trackrefund': 'Track Refund'\n",
    "}"
   ],
   "id": "3ade98f2432436a2",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T08:17:17.481132Z",
     "start_time": "2025-03-22T08:17:17.405332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Define FAQ and Non-FAQ intents\n",
    "faq_intents = [\n",
    "    'getweather', 'searchcreativework', 'greeting', 'greetingresponse',\n",
    "    'courtesygreeting', 'courtesygreetingresponse', 'currenthumanquery',\n",
    "    'namequery', 'realnamequery', 'timequery', 'thanks', 'nottalking2u',\n",
    "    'understandquery', 'shutup', 'swearing', 'goodbye', 'courtesygoodbye',\n",
    "    'whoami', 'clever', 'gossip', 'jokes', 'podbaydoor', 'podbaydoorresponse',\n",
    "    'selfaware'\n",
    "]\n",
    "\n",
    "non_faq_intents = [\n",
    "    'playmusic', 'addtoplaylist', 'ratebook', 'searchscreeningevent',\n",
    "    'bookrestaurant', 'cancelorder', 'changeorder', 'changeshippingaddress',\n",
    "    'checkcancellationfee', 'checkinvoice', 'checkpaymentmethods',\n",
    "    'checkrefundpolicy', 'complaint', 'contactcustomerservice',\n",
    "    'contacthumanagent', 'createaccount', 'deleteaccount', 'deliveryoptions',\n",
    "    'deliveryperiod', 'editaccount', 'getinvoice', 'getrefund',\n",
    "    'newslettersubscription', 'paymentissue', 'placeorder', 'recoverpassword',\n",
    "    'registrationproblems', 'review', 'setupshippingaddress', 'switchaccount',\n",
    "    'trackorder', 'trackrefund'\n",
    "]\n",
    "\n",
    "# Function to predict intent for a real-time query\n",
    "def predict_intent(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the predicted label\n",
    "    predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    # Map the predicted label back to the intent name\n",
    "    intent_name = label_encoder.inverse_transform([predicted_label])[0]\n",
    "    \n",
    "    # Convert intent name to a readable format\n",
    "    readable_intent = intent_label_mapping.get(intent_name, intent_name)\n",
    "    \n",
    "    # Classify as FAQ or Non-FAQ\n",
    "    if intent_name in faq_intents:\n",
    "        intent_type = \"FAQ\"\n",
    "    else:\n",
    "        intent_type = \"Non-FAQ\"\n",
    "    \n",
    "    return readable_intent, intent_type\n",
    "\n",
    "# Test with a real-time query\n",
    "user_query = \"hello, who are you\"\n",
    "predicted_intent, intent_type = predict_intent(user_query)\n",
    "print(f\"Predicted Intent: {predicted_intent}\")\n",
    "print(f\"Intent Type: {intent_type}\")"
   ],
   "id": "df9060c9210bee69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Intent: Name Query\n",
      "Intent Type: FAQ\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentiment Analysis",
   "id": "a250a71559751383"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T08:11:07.334421Z",
     "start_time": "2025-03-22T08:11:06.993157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the trained model and tokenizer from the specified path\n",
    "model_path = r\"C:\\Users\\LENOVO\\Desktop\\Projects\\SwiftAssist-AI\\SwiftAssist-AI\\contentsentiment_model\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "\n",
    "# Function to predict sentiment\n",
    "def predict_sentiment(text):\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=96).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**encoding)\n",
    "        probs = torch.softmax(output.logits, dim=1)\n",
    "        pred = torch.argmax(probs).item()\n",
    "    \n",
    "    return \"Positive\" if pred == 1 else \"Negative\"\n",
    "\n"
   ],
   "id": "fab3c68477443c88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T08:11:07.426658Z",
     "start_time": "2025-03-22T08:11:07.408703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test with a real-time query\n",
    "user_query = \"Hello who are you\"\n",
    "predicted_sentiment = predict_sentiment(user_query)\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ],
   "id": "7b178a55d37cb7fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FAQ answering model",
   "id": "a321428b3884b568"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T08:13:55.024877Z",
     "start_time": "2025-03-22T08:13:54.159529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test with a real-time query\n",
    "user_query = \"What is your refund policy?\"\n",
    "response = retrieve_answer(user_query, D_emb)\n",
    "print(response)  # Output: \"At the same time...\""
   ],
   "id": "43786f1b9428895c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refunds are processed within 7-10 business days after we receive the returned item.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## AI response generation",
   "id": "531cc07d76214ca2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 27,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer  # For encoding the query\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model for text processing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Load pre-trained model for encoding\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to clean and preprocess the question\n",
    "def clean_question(text):\n",
    "    # Lowercase and remove punctuation\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "\n",
    "    # Correct typos (optional)\n",
    "    words = text.split()\n",
    "    corrected_words = [spell.correction(word) if spell.correction(word) is not None else word for word in words]\n",
    "    text = ' '.join(corrected_words)\n",
    "\n",
    "    # Lemmatize and remove stopwords\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load preprocessed FAQ embeddings\n",
    "D_emb = pd.read_csv('data/FAQ Answering/Preprocessed embedding/D_emb.csv')\n",
    "\n",
    "# Convert embeddings from strings to numpy arrays\n",
    "D_emb[\"embedding\"] = D_emb[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Function to retrieve the most relevant answer\n",
    "def retrieve_answer(user_query, df, threshold=0.7):\n",
    "    # Clean the user query\n",
    "    cleaned_user_query = clean_question(user_query)\n",
    "    \n",
    "    # Generate embedding for the query\n",
    "    query_embedding = model.encode([cleaned_user_query])\n",
    "    \n",
    "    # Compute similarity with FAQ embeddings\n",
    "    similarities = cosine_similarity(query_embedding, np.stack(df[\"embedding\"]))\n",
    "    best_match_idx = np.argmax(similarities)\n",
    "    best_score = similarities[0][best_match_idx]\n",
    "\n",
    "    # Retrieve the best answer if similarity score is above the threshold\n",
    "    if best_score >= threshold:\n",
    "        return df.iloc[best_match_idx][\"cleaned_answer\"]\n",
    "    else:\n",
    "        return \"Sorry, I couldn't find a relevant answer.\"\n",
    "\n"
   ],
   "id": "1789b51825af19c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T08:17:48.904379Z",
     "start_time": "2025-03-22T08:17:48.896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "# Load API key from environment\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBTpM1obfZC1LV8f7bc99_cpAs7_JHOMRY\"\n",
    "\n",
    "# Initialize the model\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro-latest\", temperature=0.7)\n",
    "\n",
    "# Define a prompt template for intent-aware responses\n",
    "template = \"\"\"\n",
    "You are a customer support chatbot. Follow these steps:\n",
    "1. Identify the intent of this query: {query}\n",
    "   - Possible intents: FAQ, complaint, product_inquiry, technical_support.\n",
    "2. Generate a helpful response based on the intent.\n",
    "\n",
    "Respond in this format:\n",
    "Intent: [intent]\n",
    "Response: [response]\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"query\"])\n",
    "\n",
    "# Initialize the LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n"
   ],
   "id": "4e1d4e061e4e3c5c",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T08:17:52.254315Z",
     "start_time": "2025-03-22T08:17:50.782830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "user_query = \"hello, who are you\"\n",
    "try:\n",
    "    # Run the chain with the user query\n",
    "    result = chain.run(query=user_query)\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error generating response: {e}\")"
   ],
   "id": "dc2c8c6e4a7118ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent: FAQ\n",
      "Response: Hello! I'm a customer support chatbot here to help answer your questions and guide you through our services. How can I assist you today?\n",
      "\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tone adjustment",
   "id": "738e49add7f38b01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:04:20.349833Z",
     "start_time": "2025-03-22T15:04:20.289799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "# Load API key from environment\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBTpM1obfZC1LV8f7bc99_cpAs7_JHOMRY\"\n",
    "\n",
    "# Initialize the model\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro-latest\", temperature=0.7)\n",
    "\n",
    "# Sentiment analysis function (placeholder - replace with your actual sentiment analysis model)\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Placeholder function for sentiment analysis.\n",
    "    Replace this with your actual sentiment analysis model.\n",
    "    \"\"\"\n",
    "    # Example: Use a simple rule-based approach for demonstration\n",
    "    if \"happy\" in text.lower() or \"love\" in text.lower():\n",
    "        return \"positive\"\n",
    "    elif \"sad\" in text.lower() or \"angry\" in text.lower():\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Define a prompt template for intent-aware responses with tone adjustment\n",
    "template = \"\"\"\n",
    "You are a customer support chatbot. Follow these steps:\n",
    "1. Identify the intent of this query: {query}\n",
    "   - Possible intents: FAQ, complaint, product_inquiry, technical_support.\n",
    "2. Generate a helpful response based on the intent.\n",
    "3. Adjust the tone of the response based on the sentiment: {sentiment}\n",
    "   - If sentiment is positive, use a cheerful and friendly tone.\n",
    "   - If sentiment is negative, use an empathetic and supportive tone.\n",
    "   - If sentiment is neutral, use a neutral and professional tone.\n",
    "\n",
    "Respond in this format:\n",
    "Intent: [intent]\n",
    "Sentiment: [sentiment]\n",
    "Response: [response]\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"query\", \"sentiment\"])\n",
    "\n",
    "# Initialize the LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n"
   ],
   "id": "c8acdbf25135631e",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:04:22.578426Z",
     "start_time": "2025-03-22T15:04:21.070700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "user_query = \"Hello\"\n",
    "try:\n",
    "    # Analyze sentiment of the user query\n",
    "    sentiment = analyze_sentiment(user_query)\n",
    "    \n",
    "    # Run the chain with the user query and sentiment\n",
    "    result = chain.run(query=user_query, sentiment=sentiment)\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error generating response: {e}\")"
   ],
   "id": "2694c33f9176a6d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent: FAQ\n",
      "Sentiment: neutral\n",
      "Response: Hello there! How can I assist you today?\n",
      "\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d72433f4e761dba7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
